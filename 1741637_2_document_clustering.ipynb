{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Clustering text documents using k-means and dimensionality reduction\n",
    "\n",
    "\n",
    "This is an example showing how dimensionality reduction can mitigate the \"curse of dimensionality\", by denoising data and improving performance of euclidean-based clustering approaches. In this example, we cluster a set of documents, represented as bag-of-worids, using two approaches:\n",
    "1. A standard k-means algorithm (k-means++)\n",
    "2. We apply k-means after reducing the dimensionality of the space via application of a truncated SVD\n",
    "\n",
    "We use standard measures of clustering quality to compare results provided by the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "To test our ideas, we begin with some standard datasets, for which ```sklearn``` provides a class for automatic downloading and preprocessing. \n",
    "As stated in the description, \"The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\" Please refer to http://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset for more information.\n",
    "\n",
    "To begin with, we import the libraries we will be using in this notebook:\n",
    "\n",
    "## Our goal\n",
    "Given a collection containing documents from $k$ different topics ($k = 3$ in this example), we want to extract the $10$ \"most important\" terms for each topic. We will consider 3 approaches:\n",
    "1. The first is a pure clustering method: we treat each document as a point in feature space (features are the terms) and we apply a standard $k$-means algorithm. Each of the $k$ centroids thus obtained represents the \"typical\" document for the corresponding topic. We identify the terms that correspond to the $10$ largest entries.\n",
    "2. The second is a purely SVD-based approach. We compute the first $k$ components of the SVD of the data matrix, setting $k$ to the number of topics in the subcollection we downloaded ($k = 3$ in this example). The underlying assumption is that the $k$ main singular vectors are correlated with the subcollection's topics. This turns out to be true in this example, although it does not have to always be the case.\n",
    "3. The third is a hybrid approach, in which data are first projected onto a subspace in $k$ dimensions using an SVD, so that each original point $\\mathbf{x}$ now corresponds to a vector $\\hat{\\mathbf{x}}$, with $\\hat{\\mathbf{x}}_i$ the component of $\\hat{\\mathbf{x}}$ along the $i$-th right singular vector. We then apply $k$-means to the projected points. The underlying idea is that the entries of vectors in projected space, corresponding to documents on the same topic, are correlated. After performing clustering, we again have $k$ centroids in projected space. We identify the points corresponding to these centroids in the original space (see following paragraphs about how this is done) and we then proceed like in point 1. above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to proceed\n",
    "We next describe the main steps for the implementation of the solutions mentioned above. Note that we are not implementing the obvious, naive solution, that directly uses the label/topic of each document to retrieve the most important words for each topic. It could be interesting to compare the results with those of this simple solution, based on the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class ```TruncatedSVD``` implements truncated SVD. ```TfidfVectorizer``` as usual is used to convert a collection of documents, represented as a list of strings, into the corresponding tf-idf matrix. ```Normalizer``` provides utilities for vector normalization, while ```make_pipeline``` allows to pipeline two or more estimators. In our case, we will use a pipelinine consisting of a first stage performing a low-rank approximation of the dataset, followed by an application of k-means to projected data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first select some categories from the 20 newsgroups dataset. These are specified by a list of string descriptors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['talk.religion.misc', 'comp.graphics', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next download the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='all', categories=categories, \n",
    "                             shuffle=False, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, ```subset='all'``` means the dataset is downloaded in its entirety and is not split into a training and test set, which we do not need in this case. Documents are not randomly reordered (```Shuffle=False```) and we remove all metadata, leaving only body text. ```dataset``` is an object describing the dataset. Its attributes ```filenames``` and ```target``` are two arrays, respectively containing the paths to the different documents and the corresponding labels, represented as integers from ```0``` to ```len(categories) - 1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset.target\n",
    "true_k = len(np.unique(labels)) ## This should be 3 in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Point 4: Preliminary cleanup of the dataset\n",
    "\n",
    "First we find out which are the words less used"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "num_doc_threshold = 5\n",
    "\n",
    "words_stat = defaultdict(lambda: [0, 0, []])\n",
    "\n",
    "for i in range(len(dataset.data)):\n",
    "    for word in list(set(re.split('\\W+', dataset.data[i]))):\n",
    "        words_stat[word][0] += dataset.data[i].count(word)\n",
    "        words_stat[word][1] += 1\n",
    "        if words_stat[word][1] <= num_doc_threshold: words_stat[word][2].append(i)\n",
    "        else: words_stat[word][2] = None\n",
    "\n",
    "del words_stat[\"\"]\n",
    "\n",
    "df = pd.DataFrame.from_dict(words_stat, orient=\"index\", columns=[\"frequency\", \"num_doc\", \"documents\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we create an inverted index. <br>\n",
    "So that we know for each document which word we have to remove"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "doc_and_words = defaultdict(lambda: [])\n",
    "\n",
    "for word, document_ids in df[(df[\"frequency\"]<=num_doc_threshold) & (df[\"num_doc\"]<=num_doc_threshold)][\"documents\"].iteritems():\n",
    "    for doc_id in document_ids:\n",
    "        doc_and_words[doc_id].append(word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally we scan through the documents that contains those rare words and delete them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "doc_to_inspect = list(set(np.concatenate(df[(df[\"frequency\"]<=num_doc_threshold) & (df[\"num_doc\"]<=num_doc_threshold)][\"documents\"].values)))\n",
    "\n",
    "for doc_id, words in doc_and_words.items():\n",
    "    regex = re.compile(rf\"\\W{'|'.join(words)}\\W\")\n",
    "    dataset.data[doc_id] = re.sub(regex, \" \", dataset.data[doc_id])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first perform lemmatization, which seems to behave better than stemming. The reason might be that the latter is too \"aggressive\" for this collection, consisting of short documents that may contain misspells, abbreviations etc. You would probably experience similar problems with a corpus of Twitter posts. You can try with stemming after commenting the next block of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(dataset.data)):\n",
    "    word_list = word_tokenize(dataset.data[i])\n",
    "    lemmatized_doc = \"\"\n",
    "    for word in word_list:\n",
    "        lemmatized_doc = lemmatized_doc + \" \" + lemmatizer.lemmatize(word)\n",
    "    dataset.data[i] = lemmatized_doc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point 1: Use CountVectorizer instead of the Tf-Idf\n",
    "\n",
    "We next convert our corpus with CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english') ## Corpus is in English\n",
    "X = vectorizer.fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next check the data matrix shape. Note that points/documents correspond to rows, while terms correspond to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2588, 6793)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 1: using standard k-means\n",
    "We first cluster documents using the standard k-means algorithm (actually, a refined variant called k-means++), without any further date preprocessing. The key parameter of choice when performing k-means is $k$. Alas, there really is no principled way to choose an initial value for $k$. Essentially we have two options:\n",
    "\n",
    "1. We choose a value that reflects our knowledge about the data, as in this case\n",
    "2. We try several value, possibly in increasing order. We proceed this way as long as the quality of the resulting clustering (as measured by one or more quality indices) increases. We stop when it starts decreasing. As you may suspect, this case arises pretty often in practice\n",
    "\n",
    "In this specific case, we set $k = 3$ of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.260s\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next take some standard measure of cluster quality. Please refer to ```sklearn.metrics``` documentation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.004\n",
      "Completeness: 0.137\n",
      "V-measure: 0.007\n",
      "Adjusted Rand-Index: -0.001\n",
      "Silhouette Coefficient: 0.931\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, the above results are saying that we achieve some degree of clustering, but this is far from optimal. In particular, there is a certain degree of overlap among different cluster, with many pairs of documents from the same category assigned to different partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally identify the 10 most relevant terms in each cluster. Intuitively, these are the ones that correspond to entries of largest magnitude in the centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: image edu data graphics pub ftp file available format package\n",
      "Cluster 1: wa space ha like know image just time god people\n",
      "Cluster 2: jpeg image file gif color format version program quality bit\n"
     ]
    }
   ],
   "source": [
    "centroids = km.cluster_centers_.argsort()[:, ::-1] ## Indices of largest centroids' entries in descending order\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 2: Using singular vectors\n",
    "We next project data onto the subspace(s) corresponding to the $k$ main components ($k = 3$ in this case). We then identify the words corresponding to the $10$ largest components (with sign) of the right singular vector. The reason for using right singular vectors is that the $i$-th such vector is the representation of the $i$-th topic in feature space (features are terms in our case). We project input data onto a lower dimensional space using ```TruncatedSVD```. We first check the number of dimensions/features in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data have 6793 dimensions/features/terms\n"
     ]
    }
   ],
   "source": [
    "print(\"The original data have\", X.shape[1], \"dimensions/features/terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next choose the number of dimensions in projected space, i.e., the number of singular values (equivalently, the number of singular vector pairs) we intend to retain. Here, we can proceed in a more or less principled way:\n",
    "\n",
    "1. The first way is experimenting with different, increasing values for the number of dimensions, until experimental results (in our case, cluster quality after application of k-means) are satisfactory\n",
    "2. The second, more principled approach is choosing a value $r$ for the number of dimensions to retain, such that $\\sum_{i=1}^r\\sigma_i^2\\ge \\alpha\\sum_{i=1}^{\\min\\{m, n\\}}\\sigma_i^2$. In practice, we do not want to compute all singular values in advance, even though this is cheaper than also computing singular vectors. Another heuristic approach is stopping the first time that $\\sigma_{i+1}/\\sigma_i\\le\\beta$, for a suitably small $\\beta$\n",
    "3. Finally, if clusters are well-separated, the first $k$ singular vectors will themselves reflect cluster structure. In this case, one can try to take $r = k$. \n",
    "\n",
    "In our case we use $r = 3$, with the idea that the $k$ main singular vector correspond the main topics of our collection. We get a good match, but we can do better (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Point 2: Explained variance\n",
    "\n",
    "we take all the singular values for the moment, we want to see how the explained variance behaves."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.527698s, 100 singular values\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# in order to have a better view we plot only the first 100 singular values\n",
    "r = 100\n",
    "t0 = time()\n",
    "svd = TruncatedSVD(r)\n",
    "svd.fit(X)\n",
    "print(\"done in %fs, %i singular values\" % (time() - t0, r))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(svd.explained_variance_ratio_.cumsum())\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Point 3: Centering the dataset before SVD\n",
    "\n",
    "before apply the SVD on the dataset ve center it, will be like performing the PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.034527s\n"
     ]
    }
   ],
   "source": [
    "r = true_k\n",
    "t0 = time()\n",
    "svd = TruncatedSVD(r)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "centered_X = X - X.mean(axis=0)\n",
    "Y = lsa.fit_transform(centered_X)\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "var = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? Consider the original tf-idf matrix (i.e., ```X``` before application of ```TruncatedSVD```). Let us call $X$ this matrix. We have $X = U\\Sigma V^T$. By computing ```TruncatedSVD```, we only kept the first $r$ singular values/singular vector pairs. How did we project? Simply put, we projected the row corresponding to each document onto the subspace spanned by the first $r\\ (= k)$ right singular vectors. More in detail, note that right-multiplying both members of the equation above by $V_r$ gives $XV_r = U_r\\Sigma_r$. Hence, the projection $\\hat{X}_{j*}$ of the $j-th$ row (document) of $X$ onto the \"concept space\" is given by:\n",
    "\n",
    "$$\\hat{X}_{j*} = X_{j*}V_r$$\n",
    "\n",
    "Note that, consistently, $\\hat{X}_{j*}$ is now a vector with $r$ components. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents is still 2588\n",
      "The number of dimension has become 3\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of documents is still\", Y.shape[0])\n",
    "print(\"The number of dimension has become\", Y.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most important features/terms in every topic (first try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "image jpeg file format gif color edu program version available \n",
      "Topic 1: \n",
      "edu pub data graphics ftp mail 128 ray 3d send \n",
      "Topic 2: \n",
      "jehovah god elohim lord father christ mcconkie wa jesus son \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    s = \"\"\n",
    "    for t in sorted_terms:\n",
    "        s += t[0] + \" \"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD seems to have done a very good job (better than $k$-means) for topics 1 and 2, while the most representative words for topic 0 are far less impressive. Maybe, more information can be extracted if we consider that singular vectors are defined up to their signs. In particular, consider the SVD expansion of a given matrix $A$:\n",
    "$$\n",
    "    A = \\sum_i\\sigma_i\\mathbf{u}_i\\mathbf{v}_i^T\n",
    "$$\n",
    "The generic, $i$-th projector $\\mathbf{u}_i\\mathbf{v}_i$ does not change if you consider the singular vectors $-\\mathbf{u}_i$ and $-\\mathbf{v}_i$ (notice that you have to change signs to both!). Signs, in some sense, express degrees of correlation. In our case, since the original points have non-negative entries (they are term frequencies), negative entries on a right singular vector's component corresponding to a given term denotes a negative correlation of the corresponding topic with the term. Let us try to put this intuition in practice, by considering, for each singular vectors, the most important terms if we consider both ascending and descending ordering of the entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "image jpeg file format gif color edu program version available \n",
      "koresh kent moral den cheers p2 p3 p1 oh morality \n",
      "Topic 1: \n",
      "edu pub data graphics ftp mail 128 ray 3d send \n",
      "jpeg gif color quality file viewer jfif bit setting version \n",
      "Topic 2: \n",
      "jehovah god elohim lord father christ mcconkie wa jesus son \n",
      "edu graphics pub mail ftp 128 image data 3d ray \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(svd.components_.shape[0]):\n",
    "    terms_comp = [[terms[j], svd.components_[i][j]] for j in range(svd.components_.shape[1])]\n",
    "    asc_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    des_terms = sorted(terms_comp, key= lambda x:x[1], reverse=False)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    s = \"\"\n",
    "    for t in asc_terms:\n",
    "        s += t[0] + \" \"\n",
    "    print(s)\n",
    "    s = \"\"\n",
    "    for t in des_terms:\n",
    "        s += t[0] + \" \"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are now impressive again. Apparently, the \"space\" topic can be obtained by considering the words that are less correlated with the topic \"religion\", which also seems to be negatively correlated with \"computer graphics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 3: Clustering projected data\n",
    "We next use again k-means, but we first project the input data onto a lower dimensional space using ```TruncatedSVD```. We first check the number of dimensions/features in the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now apply k-means to the projected data and collect quality statistics"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100)\n",
    "t0 = time()\n",
    "km.fit(Y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(Y, km.labels_, sample_size=1000))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.130s\n",
      "\n",
      "Homogeneity: 0.060\n",
      "Completeness: 0.108\n",
      "V-measure: 0.078\n",
      "Adjusted Rand-Index: 0.021\n",
      "Silhouette Coefficient: 0.692\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next identify the terms that are most relevant in each cluster, as identified by applying ```TruncatedSVD``` first and k-means afterwards. Note that we face a problem here. Centroids are now in projected space and thus each have $r$ components, corresponding to the dimensions of the concept space and hard. To verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(km.cluster_centers_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, correctly we have $k$ centroids, each with $r$ coordinates. In some way, we need to \"project\" these back to the original space. We argue in the following way. Assume $\\hat{C}_j$ is the $j$-th centroid. We look for a vector $C_j$ in the original space that yields $\\hat{C}_j$ when projected onto the concept space. Note that vectors in the original space have $n$ features (with $n$ the number of terms in our case), so they are projected using the *right* singular vectors. As a result, $C_j$ has to satisfy the following equality:\n",
    "\n",
    "$$\\hat{C_j}^T = C_j^TV_r$$\n",
    "\n",
    "Right-multiplying both members by $V^T$ we obtain $C_j^T = \\hat{C}_j^TV_r^T$. Note that $V_rV_r^T$ is diagonal and its number of components is consistent with the number of components $C_j$ is supposed to have. In our case, this very operation is performed by the method ```TruncatedSVD.inverse_transform()```. Let us check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6793)\n"
     ]
    }
   ],
   "source": [
    "original_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "print(original_centroids.shape) ## Just a sanity check\n",
    "for i in range(original_centroids.shape[0]):\n",
    "    original_centroids[i] = np.array([x for x in original_centroids[i]])\n",
    "svd_centroids = original_centroids.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally identify the 10 most relevant terms for each centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: jehovah god elohim lord father christ mcconkie wa jesus son\n",
      "Cluster 1: image edu file graphics pub data ftp format available mail\n",
      "Cluster 2: cheers hello den advance p3 p2 kent p1 prize detector\n"
     ]
    }
   ],
   "source": [
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in svd_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we get are in some way intermediate between ones obtained with the first two approaches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}